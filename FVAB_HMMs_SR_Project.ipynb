{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "FVAB_HMMs_SR_Project.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<b> Definizione di tutti gli import necessari e dei path utilizzati durante le varie fasi </b>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import librosa as lb\n",
    "import librosa.display as display\n",
    "import librosa.effects as effects\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import Audio\n",
    "\n",
    "from hmmlearn import hmm\n",
    "from collections import Counter\n",
    "\n",
    "import sys\n",
    "n_cluster = 1000\n",
    "\n",
    "\n",
    "BASE_TRAINING_PATH=\"C:\\\\Users\\\\alexp\\\\Desktop\\\\Voxceleb_wav\\\\wav\\\\\"\n",
    "#BASE_TRAINING_PATH=\"/home/axel1143/Scrivania/Voxceleb_wav/wav/\"\n",
    "BASE_PATH = \"C:\\\\Users\\\\alexp\\\\Desktop\\\\Voxceleb_mat\\\\\"\n",
    "#BASE_PATH = \"/home/alex/Scrivania/Voxceleb_mat/\""
   ],
   "metadata": {
    "id": "mKs7zKLIw8eK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b> Definizione della funzione di pre-processing in due passi: </b>\n",
    "\n",
    "1) Fase di pre-enfasi con a = 0.97\n",
    "2) Fase di riduzione dei momenti di silenzio con threshold = 25db"
   ],
   "metadata": {
    "id": "mNa_-OsWxMjb",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "def pre_processing(path):\n",
    "  #load\n",
    "  audio, sr = lb.load(path = path, mono = True)\n",
    "\n",
    "  #pre-emp\n",
    "  pre_emp_audio = effects.preemphasis(audio, 0.97)\n",
    "\n",
    "\n",
    "  #removing silence in audio\n",
    "  clips = effects.split(pre_emp_audio, top_db=25)\n",
    "  silenced=[]\n",
    "  for c in clips:\n",
    "    data = audio[c[0]: c[1]]\n",
    "    silenced.extend(data)\n",
    "\n",
    "  silence = np.array(silenced)\n",
    "\n",
    "\n",
    "\n",
    "  return silence, sr\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b> Creazione del dataset: </b>\n",
    "\n",
    "1) Applicazione del pre-processing ad ogni audio\n",
    "2) Estrazione dei coefficienti MFCC\n",
    "3) Estrazione dei Delta\n",
    "4) Creazione del file .csv\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "header = [\"id\",\"path\",\n",
    "          \"mfcc1\",\"mfcc2\",\"mfcc3\",\"mfcc4\",\"mfcc5\",\"mfcc6\",\"mfcc7\",\"mfcc8\",\"mfcc9\",\"mfcc10\",\"mfcc11\",\"mfcc12\",\"mfcc13\",\n",
    "          \"delta1\",\"delta2\",\"delta3\",\"delta4\",\"delta5\",\"delta6\",\"delta7\",\"delta8\",\"delta9\",\"delta10\",\"delta11\",\"delta12\",\"delta13\"]\n",
    "\n",
    "counter_speaker = 0\n",
    "final_matrix = []\n",
    "\n",
    "for dir in os.listdir(BASE_TRAINING_PATH):\n",
    "    if counter_speaker < 300:\n",
    "        counter_speaker+=1\n",
    "        count_audio = 0\n",
    "        for dir1 in os.listdir(BASE_TRAINING_PATH+dir):\n",
    "            if count_audio < 40:\n",
    "                for file in os.listdir(BASE_TRAINING_PATH+dir+\"/\"+dir1):\n",
    "                  if count_audio < 40:\n",
    "                      if not os.path.isdir(BASE_TRAINING_PATH+dir+\"/\"+dir1+\"/\"+file):\n",
    "                        count_audio +=1\n",
    "                        COMPLETE_PATH = BASE_TRAINING_PATH+dir+\"/\"+dir1+\"/\"+file\n",
    "                        processed_audio, sr = pre_processing(COMPLETE_PATH)  ## 1) applicazione pre-processing\n",
    "\n",
    "                        mfcc = lb.feature.mfcc(y = processed_audio, sr = sr, n_mfcc=13)  ## 2) estrazione mfcc\n",
    "                        mfcc_delta = lb.feature.delta(mfcc) ## 3) estrazione delta\n",
    "\n",
    "                        row_csv = []\n",
    "                        row_csv.append(dir)\n",
    "                        row_csv.append(COMPLETE_PATH)\n",
    "\n",
    "\n",
    "                        for row in mfcc:\n",
    "                          row_csv_2 = []\n",
    "                          for value in row:\n",
    "                            row_csv_2.append(value)\n",
    "                          row_csv.append(row_csv_2)\n",
    "\n",
    "                        for row in mfcc_delta:\n",
    "                          row_csv_2 = []\n",
    "                          for value in row:\n",
    "                            row_csv_2.append(value)\n",
    "                          row_csv.append(row_csv_2)\n",
    "\n",
    "\n",
    "                        final_matrix.append(row_csv)   ## 4) Creazione csv\n",
    "                        print(\"audio n°\", count_audio, \"done\")\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "df = pd.DataFrame(np.array(final_matrix))\n",
    "df.columns = header\n",
    "df.to_csv(BASE_PATH + \"featuring_extraction.csv\" )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b> Creazione della matrice necessaria per la fase di Vector Quantization. </b>\n",
    "\n",
    "La matrice sará del tipo:\n",
    "\n",
    "|         | MFCC_1 | MFCC_2 | ... | MFCC_13 |\n",
    "|---------|--------|--------|-----|---------|\n",
    "| FRAME1  |        |        |     |         |\n",
    "| FRAME2  |        |        |     |         |\n",
    "| FRAME3  |        |        |     |         |\n",
    "| FRAME.. |        |        |     |         |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "La matrice conterrá in successione, tutti i frame di tutti gli audio del dataset, quindi il numero di righe sará:\n",
    "$\n",
    "#ROW = \\sum^{300}_{i=0} \\sum^{40}_{k=0} frame(k, i)\n",
    "$\n",
    "con\n",
    "1) i = speaker corrente\n",
    "2) k = audio corrente\n",
    "3) frame(i, k) = numero frame audio k, speaker i\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(BASE_PATH + \"featuring_extraction.csv\")\n",
    "\n",
    "features_mfcc = [\"mfcc1\", \"mfcc2\", \"mfcc3\", \"mfcc4\", \"mfcc5\", \"mfcc6\", \"mfcc7\", \"mfcc8\", \"mfcc9\", \"mfcc10\", \"mfcc11\",\n",
    "            \"mfcc12\", \"mfcc13\"]\n",
    "\n",
    "labels = df.loc[:, [\"id\"]].values #labael dataset\n",
    "features_mfcc_values = df.loc[:, features_mfcc].values #mfcc dataset\n",
    "\n",
    "total = features_mfcc_values.shape[0] * features_mfcc_values.shape[1]\n",
    "counter = 0\n",
    "correct_dataset_mfcc = []\n",
    "header = []\n",
    "final_matrix = []\n",
    "frames = []\n",
    "\n",
    "for i in range(features_mfcc_values.shape[0]): #row_dataset.append(labels[i])\n",
    "    correct_dataset_mfcc = []\n",
    "    for j in range(features_mfcc_values.shape[1]):\n",
    "        row_dataset = []\n",
    "        features_mfcc_values[i][j] = features_mfcc_values[i][j].split()\n",
    "        for k in range(len(features_mfcc_values[i][j])):\n",
    "            features_mfcc_values[i][j][k] = features_mfcc_values[i][j][k].replace(\"[\", \"\")\n",
    "            features_mfcc_values[i][j][k] = features_mfcc_values[i][j][k].replace(\"]\", \"\")\n",
    "            features_mfcc_values[i][j][k] = features_mfcc_values[i][j][k].replace(\",\", \"\")\n",
    "        for ele in features_mfcc_values[i][j]:\n",
    "            if ele == \"\":\n",
    "                features_mfcc_values[i][j].remove(ele)\n",
    "        for k in range(len(features_mfcc_values[i][j])):\n",
    "            features_mfcc_values[i][j][k] = float(features_mfcc_values[i][j][k])\n",
    "            row_dataset.append(features_mfcc_values[i][j][k])\n",
    "        correct_dataset_mfcc.append(row_dataset)\n",
    "    counter += 1\n",
    "\n",
    "    transposed = np.array(correct_dataset_mfcc).transpose()\n",
    "    frames.append(transposed.shape[0])\n",
    "    for row in transposed:\n",
    "        final_matrix.append(row)\n",
    "\n",
    "    print(counter, \"/\", features_mfcc_values.shape[0])\n",
    "\n",
    "\n",
    "pd.DataFrame(final_matrix).to_csv(BASE_PATH + \"preparing_csv_mfcc.csv\", header=None, index=None)\n",
    "pd.DataFrame(frames).to_csv(BASE_PATH + \"frame_size_mfcc.csv\", header=None, index=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b> Ripetuto processo precedente anche per coefficienti delta </b>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Creating delta matrix\n",
    "df = pd.read_csv(BASE_PATH + \"featuring_extraction.csv\")\n",
    "\n",
    "features_delta = [\"delta1\", \"delta2\", \"delta3\", \"delta4\", \"delta5\", \"delta6\", \"delta7\", \"delta8\", \"delta9\", \"delta10\", \"delta11\",\n",
    "            \"delta12\", \"delta13\"]\n",
    "\n",
    "labels = df.loc[:, [\"id\"]].values #labael dataset\n",
    "features_delta_values = df.loc[:, features_delta].values #mfcc dataset\n",
    "\n",
    "total = features_delta_values.shape[0] * features_delta_values.shape[1]\n",
    "counter = 0\n",
    "correct_dataset_mfcc = []\n",
    "header = []\n",
    "final_matrix = []\n",
    "frames = []\n",
    "\n",
    "for i in range(features_delta_values.shape[0]): #row_dataset.append(labels[i])\n",
    "    correct_dataset_mfcc = []\n",
    "    for j in range(features_delta_values.shape[1]):\n",
    "        row_dataset = []\n",
    "        features_delta_values[i][j] = features_delta_values[i][j].split()\n",
    "        for k in range(len(features_delta_values[i][j])):\n",
    "            features_delta_values[i][j][k] = features_delta_values[i][j][k].replace(\"[\", \"\")\n",
    "            features_delta_values[i][j][k] = features_delta_values[i][j][k].replace(\"]\", \"\")\n",
    "            features_delta_values[i][j][k] = features_delta_values[i][j][k].replace(\",\", \"\")\n",
    "        for ele in features_delta_values[i][j]:\n",
    "            if ele == \"\":\n",
    "                features_delta_values[i][j].remove(ele)\n",
    "        for k in range(len(features_delta_values[i][j])):\n",
    "            features_delta_values[i][j][k] = float(features_delta_values[i][j][k])\n",
    "            row_dataset.append(features_delta_values[i][j][k])\n",
    "        correct_dataset_mfcc.append(row_dataset)\n",
    "    counter += 1\n",
    "\n",
    "    transposed = np.array(correct_dataset_mfcc).transpose()\n",
    "    frames.append(transposed.shape[0])\n",
    "    for row in transposed:\n",
    "        final_matrix.append(row)\n",
    "\n",
    "    print(counter, \"/\", features_delta_values.shape[0])\n",
    "\n",
    "\n",
    "pd.DataFrame(final_matrix).to_csv(BASE_PATH + \"preparing_csv_delta.csv\", header=None, index=None)\n",
    "pd.DataFrame(frames).to_csv(BASE_PATH + \"frame_size_delta.csv\", header=None, index=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b> Fase di Vector Quantization </b>\n",
    "\n",
    "Operata tramite l'algoritmo K-Means sia per la matrice MFCC che la matrice Delta."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "mfcc = np.array(pd.read_csv(BASE_PATH + \"preparing_csv_mfcc.csv\", header=None))\n",
    "\n",
    "model = KMeans(n_clusters=n_cluster)\n",
    "model.fit(mfcc)\n",
    "\n",
    "print(model.labels_)\n",
    "print(len(model.labels_))\n",
    "\n",
    "labels = model.labels_\n",
    "\n",
    "pd.DataFrame(labels).to_csv(BASE_PATH + \"k\" + str(n_cluster)+ \"\\\\mfcc\\\\labels_quantization.csv\", header=None, index=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "mfcc = np.array(pd.read_csv(BASE_PATH + \"preparing_csv_delta.csv\", header=None))\n",
    "\n",
    "model = KMeans(n_clusters=n_cluster)\n",
    "model.fit(mfcc)\n",
    "\n",
    "print(model.labels_)\n",
    "print(len(model.labels_))\n",
    "\n",
    "labels = model.labels_\n",
    "\n",
    "pd.DataFrame(labels).to_csv(BASE_PATH + \"k\" + str(n_cluster)+ \"\\\\delta\\\\labels_quantization_delta.csv\", header=None, index=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b> Creazione della matrice che conterrá i frame mfcc, rappruppati secondo la fase di Cluster precedente </b>\n",
    "\n",
    "Ogni riga conterrá l'audio di riferimento, ogni colonna il frame di riferimento per quell'audio, ogni cella conterrá il raggruppamento effettuato dal k-means per il relativo frame del relativo audio.\n",
    "\n",
    "|           | FRAME 1 | FRAME 2 | FRAME 3 | ... | FRAME_N |\n",
    "|-----------|---------|---------|---------|-----|---------|\n",
    "| AUDIO1    | 0       | 5       | 4       |     | 564     |\n",
    "| AUDIO2    | 24      | 2       | 124     |     | 998     |\n",
    "| AUDIO...  |         |         |         |     |         |\n",
    "| AUDIO1200 | 34      | 564     | 121     |     | 123     |"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "frames = np.array(pd.read_csv(BASE_PATH + \"frame_size_mfcc.csv\", header= None)).reshape((1, -1))\n",
    "labels = np.array(pd.read_csv(BASE_PATH + \"k\" + str(n_cluster)+ \"\\\\mfcc\\\\labels_quantization_mfcc.csv\", header=None)).reshape((1, -1))\n",
    "\n",
    "frames = np.array(frames[0])\n",
    "labels = np.array(labels[0])\n",
    "vector_complete = []\n",
    "\n",
    "prec = 0\n",
    "\n",
    "for index in frames:\n",
    "    vector_complete.append(labels[prec:prec+index])\n",
    "    prec += index\n",
    "\n",
    "vector_complete = np.array(pd.DataFrame(vector_complete))\n",
    "\n",
    "index = 0\n",
    "for row in vector_complete:\n",
    "    row_counter = 0\n",
    "    for i in range(len(row)):\n",
    "        if np.isnan(row[i]):\n",
    "            row[i] = row[row_counter]\n",
    "            row_counter+=1\n",
    "    index += 1\n",
    "    #print(index)\n",
    "print(vector_complete)\n",
    "\n",
    "pd.DataFrame(vector_complete).to_csv(BASE_PATH + \"k\" + str(n_cluster)+ \"\\\\mfcc\\\\matrix_quantization_filled.csv\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b> Divisione della matrice mfcc in train-set e test-set </b>\n",
    "\n",
    "Suddivisione audio <b> per ogni speaker </b>:\n",
    "1) Train-set = 35 file audio\n",
    "2) Test-set = 5 file audio\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observations = np.array(pd.read_csv(BASE_PATH + \"k\" + str(n_cluster)+ \"\\\\mfcc\\\\matrix_quantization_filled.csv\"))\n",
    "observations = np.delete(observations, 0, axis=1)\n",
    "\n",
    "index = 0\n",
    "train_set = []\n",
    "test_set = []\n",
    "for row in observations:\n",
    "    if 0 <= index <= 34:\n",
    "        train_set.append(row)\n",
    "        index+=1\n",
    "    elif index <= 38:\n",
    "        test_set.append(row)\n",
    "        index+=1\n",
    "    elif index == 39:\n",
    "        test_set.append(row)\n",
    "        index = 0\n",
    "\n",
    "test_set = np.array(test_set)\n",
    "train_set = np.array(train_set)\n",
    "\n",
    "\n",
    "pd.DataFrame(train_set).to_csv(BASE_PATH + \"k\" + str(n_cluster)+ \"\\\\mfcc\\\\train_set_hmm.csv\")\n",
    "pd.DataFrame(test_set).to_csv(BASE_PATH + \"k\" + str(n_cluster)+ \"\\\\mfcc\\\\test_set_hmm.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b> Creazione della matrice che conterrá i frame delta, rappruppati secondo la fase di Cluster precedente </b>\n",
    "\n",
    "<i> Fase effettuata in maniera identica alla precedente, ma per i coefficienti delta, e non gli mfcc </i>\n",
    "\n",
    "Ogni riga conterrá l'audio di riferimento, ogni colonna il frame di riferimento per quell'audio, ogni cella conterrá il raggruppamento effettuato dal k-means per il relativo frame del relativo audio.\n",
    "\n",
    "|           | FRAME 1 | FRAME 2 | FRAME 3 | ... | FRAME_N |\n",
    "|-----------|---------|---------|---------|-----|---------|\n",
    "| AUDIO1    | 0       | 5       | 4       |     | 564     |\n",
    "| AUDIO2    | 24      | 2       | 124     |     | 998     |\n",
    "| AUDIO...  |         |         |         |     |         |\n",
    "| AUDIO1200 | 34      | 564     | 121     |     | 123     |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "frames = np.array(pd.read_csv(BASE_PATH + \"frame_size_delta.csv\", header= None)).reshape((1, -1))\n",
    "labels = np.array(pd.read_csv(BASE_PATH + \"k\" + str(n_cluster)+ \"\\\\delta\\\\labels_quantization_delta.csv\", header=None)).reshape((1, -1))\n",
    "\n",
    "frames = np.array(frames[0])\n",
    "labels = np.array(labels[0])\n",
    "vector_complete = []\n",
    "\n",
    "prec = 0\n",
    "\n",
    "for index in frames:\n",
    "    vector_complete.append(labels[prec:prec+index])\n",
    "    prec += index\n",
    "\n",
    "vector_complete = np.array(pd.DataFrame(vector_complete))\n",
    "\n",
    "index = 0\n",
    "for row in vector_complete:\n",
    "    row_counter = 0\n",
    "    for i in range(len(row)):\n",
    "        if np.isnan(row[i]):\n",
    "            row[i] = row[row_counter]\n",
    "            row_counter+=1\n",
    "    index += 1\n",
    "    #print(index)\n",
    "print(vector_complete)\n",
    "\n",
    "pd.DataFrame(vector_complete).to_csv(BASE_PATH + \"k\" + str(n_cluster)+ \"\\\\delta\\\\matrix_quantization_filled_delta.csv\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b> Divisione della matrice delta in train-set e test-set </b>\n",
    "\n",
    "Suddivisione audio <b> per ogni speaker </b>:\n",
    "1) Train-set = 35 file audio\n",
    "2) Test-set = 5 file audio\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observations = np.array(pd.read_csv(BASE_PATH + \"k\" + str(n_cluster)+ \"\\\\delta\\\\matrix_quantization_filled_delta.csv\"))\n",
    "observations = np.delete(observations, 0, axis=1)\n",
    "\n",
    "index = 0\n",
    "train_set = []\n",
    "test_set = []\n",
    "for row in observations:\n",
    "    if 0 <= index <= 34:\n",
    "        train_set.append(row)\n",
    "        index+=1\n",
    "    elif index <= 38:\n",
    "        test_set.append(row)\n",
    "        index+=1\n",
    "    elif index == 39:\n",
    "        test_set.append(row)\n",
    "        index = 0\n",
    "\n",
    "test_set = np.array(test_set)\n",
    "train_set = np.array(train_set)\n",
    "\n",
    "\n",
    "pd.DataFrame(train_set).to_csv(BASE_PATH + \"k\" + str(n_cluster)+ \"\\\\delta\\\\train_set_hmm_delta.csv\")\n",
    "pd.DataFrame(test_set).to_csv(BASE_PATH + \"k\" + str(n_cluster)+ \"\\\\delta\\\\test_set_hmm_delta.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b> Creazione delle matrici di transizione per ognuno dei 300 Modelli di Markov </b>\n",
    "\n",
    "La creazione di ognuna delle 300 matrici, avviene seguendo la sequenza temporale creata dal k-means. Ogni raggruppamento di frame, operato dal k-means, rappresenterá uno stato della matrice di transizione e l'intera sequenza rappresenta una sequenza di stati.\n",
    "Ciascun modello apprenderá le probabilitá di transizione, analizzando i 35 audio destinati al relativo speaker nel trainset.\n",
    "\n",
    "Inoltre verrá prodotta anche un array di probabilitá iniziali, che stimerá per ciascuno speaker la probabilitá che una determinata sequenza inizi con un determinato stato."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "quantization_matrix = pd.read_csv(BASE_PATH +\"k\" + str(n_cluster)+ \"\\\\mfcc\\\\train_set_hmm_mfcc.csv\")\n",
    "quantization_matrix = np.array(quantization_matrix, dtype=int)\n",
    "quantization_matrix = np.delete(quantization_matrix, 0, 1)\n",
    "\n",
    "## building transition matrix\n",
    "for person in range(300):\n",
    "    starting_matrix = np.zeros((n_cluster,))\n",
    "    observation_matrix = np.zeros((n_cluster, n_cluster))\n",
    "\n",
    "    for audio_person in range(35):\n",
    "\n",
    "        actual_audio = quantization_matrix[person*35 + audio_person]\n",
    "        for frame in range(2806-1):\n",
    "\n",
    "            if frame == 0:\n",
    "                starting_matrix[actual_audio[frame]] +=1/35\n",
    "\n",
    "            observation_matrix[actual_audio[frame]][actual_audio[frame+1]] += 1\n",
    "\n",
    "    for row in observation_matrix:\n",
    "        sum_row = sum(row)\n",
    "        for col in range(len(row)):\n",
    "            row[col] = row[col] / sum_row\n",
    "\n",
    "\n",
    "    observation_matrix = pd.DataFrame(observation_matrix).fillna(0)\n",
    "    observation_matrix.to_csv(BASE_PATH + \"k\" + str(n_cluster)+ \"\\\\mfcc\\\\matrix\\\\quan_mat_\" + str(person) +\".csv\", header=None, index=None)\n",
    "    starting_matrix = pd.DataFrame(starting_matrix).fillna(0)\n",
    "    starting_matrix.to_csv(BASE_PATH + \"k\" + str(n_cluster)+ \"\\\\mfcc\\\\matrix\\\\start_mat_\" + str(person) +\".csv\", header=None, index=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b> Creazione delle matrici di transizione per ognuno dei 300 Modelli di Markov </b>\n",
    "\n",
    "Lo stesso processo avviene per la matrice di coefficienti delta."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "quantization_matrix = pd.read_csv(BASE_PATH +\"k\" + str(n_cluster)+ \"\\\\delta\\\\train_set_hmm_delta.csv\")\n",
    "quantization_matrix = np.array(quantization_matrix, dtype=int)\n",
    "quantization_matrix = np.delete(quantization_matrix, 0, 1)\n",
    "\n",
    "## building transition matrix\n",
    "for person in range(300):\n",
    "    starting_matrix = np.zeros((n_cluster,))\n",
    "    observation_matrix = np.zeros((n_cluster, n_cluster))\n",
    "\n",
    "    for audio_person in range(35):\n",
    "\n",
    "        actual_audio = quantization_matrix[person*35 + audio_person]\n",
    "        for frame in range(2806-1):\n",
    "\n",
    "            if frame == 0:\n",
    "                starting_matrix[actual_audio[frame]] +=1/35\n",
    "\n",
    "            observation_matrix[actual_audio[frame]][actual_audio[frame+1]] += 1\n",
    "\n",
    "    for row in observation_matrix:\n",
    "        sum_row = sum(row)\n",
    "        for col in range(len(row)):\n",
    "            row[col] = row[col] / sum_row\n",
    "\n",
    "\n",
    "    observation_matrix = pd.DataFrame(observation_matrix).fillna(0)\n",
    "    observation_matrix.to_csv(BASE_PATH + \"k\" + str(n_cluster)+ \"\\\\delta\\\\matrix\\\\quan_mat_\" + str(person) +\".csv\", header=None, index=None)\n",
    "    starting_matrix = pd.DataFrame(starting_matrix).fillna(0)\n",
    "    starting_matrix.to_csv(BASE_PATH + \"k\" + str(n_cluster)+ \"\\\\delta\\\\matrix\\\\start_mat_\" + str(person) +\".csv\", header=None, index=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b> Definizione della funzione di calcolo delle probabilitá </b>\n",
    "<i>Input</i>:\n",
    "1) observation matrix dello speaker associato\n",
    "2) starting probability matrix dello speaker associato\n",
    "3) sequenza di cui si cerca la somiglianza col modello\n",
    "4) frame attuale, necessario per il calcolo della probabilitá di partenza\n",
    "\n",
    "<i>Output</i>:\n",
    "Probabilitá logaritmica che la sequenza assomigli al modello dato in input.\n",
    "\n",
    "<i>Precisazioni</i>:\n",
    "Se la sequenza dovesse iniziare o dovesse muoversi in uno stato mai visto dal modello di Markov, la probabilitá del relativo movimento sará $ P(M) = \\frac{1}{\\text{#stati}} $"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_probability(observation_matrix, starting_matrix, sequence, ac_frame):\n",
    "    prob = 1\n",
    "    for i in range(len(sequence)-1):\n",
    "        if ac_frame == 0:\n",
    "            starting_prob = starting_matrix[sequence[0]]\n",
    "            if starting_prob == 0:\n",
    "                starting_prob = 1/len(sequence)\n",
    "            prob = prob * starting_prob\n",
    "\n",
    "        mov_prov = observation_matrix[sequence[i]][sequence[i+1]]\n",
    "        if mov_prov == 0:\n",
    "            mov_prov = 1/observation_matrix.shape[0]\n",
    "        prob = prob * mov_prov\n",
    "\n",
    "    return np.log(prob)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b> Fase di testing dei modelli </b>\n",
    "\n",
    "La fase di testing avviene piú volte con un numero di speaker in analisi che varia tra $[2, 3, 5, 10, 50]$\n",
    "\n",
    "\n",
    "Inoltre, la fase di testing avviene provando successivamente tutte le combinazioni (in sequenza) del numero di speaker scelto in analisi.\n",
    "Es.\n",
    "-2 speaker => Coppie di speaker = (1, 2) , (3, 4) , (5, 6) ... etc.\n",
    "-3 speaker => Terne di speaker = (1, 2, 3) , (4, 5, 6) , (7, 8, 9) ... etc.\n",
    "\n",
    "\n",
    "\n",
    "1) Per ogni sequenza, corrispondente ad un audio del test-set, verrá calcolato lo score di somiglianza relativo a tutti i modelli in analisi.\n",
    "2) Ogni sequenza viene divisa in piú finestre contenenti ognuna 20 frame del singolo audio.\n",
    "3) Su ogni finestra viene computato lo score di somiglianza con tutti i modelli in analisi, lo score piú alto verrá scelto come prediction sulla singola finestra.\n",
    "4) La fase di calcolo delle prediction, viene effettuata per ogni finestra di ogni sequenza relativa alla martrice mfcc, sia per ogni sequenza relativa alla matrice delta.\n",
    "5) Tutte le prediction (mfcc, delta) vengono raggruppate e viene scelta come predction finale quella con maggior occorrenza (moda)\n",
    "6) Viene poi successivamente calcolata la media delle accuracy per ogni coppia/terna etc di speaker in analisi.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "value_nspeaker = []\n",
    "for person in [2, 3, 5, 10, 50]: ## numero speaker in analisi\n",
    "    mean = []\n",
    "\n",
    "    for shift in range(0, 300-person, person): ## scelta della t-pla di speaker\n",
    "        correct_label = []\n",
    "        for i in range(person):\n",
    "            for j in range(5):\n",
    "                correct_label.append(i)\n",
    "\n",
    "        matrix = []\n",
    "        starting_matrix = []\n",
    "\n",
    "        matrix_delta = []\n",
    "        starting_matrix_delta = []\n",
    "\n",
    "        actual = 0\n",
    "        for i in range(shift, shift+person): ## caricate le observation matrix mfcc e le starting matrix mfcc relative agli speaker in esame\n",
    "            matrix.append(np.array(pd.read_csv(BASE_PATH + \"k\" + str(n_cluster)+ \"\\\\mfcc\\\\matrix\\\\quan_mat_\"+str(i) + \".csv\", header=None), dtype=float))\n",
    "            starting_matrix.append(np.array(pd.read_csv(BASE_PATH + \"k\" + str(n_cluster)+ \"\\\\mfcc\\\\matrix\\\\start_mat_\"+str(+i)+\".csv\", header=None), dtype=float))\n",
    "\n",
    "        test_set_mfcc = np.array(pd.read_csv(BASE_PATH +\"k\" + str(n_cluster)+ \"\\\\mfcc\\\\test_set_hmm_mfcc.csv\"), dtype=int) ## caricato il test-set mfcc\n",
    "        test_set_mfcc = np.delete(test_set_mfcc, 0, axis=1)\n",
    "\n",
    "        for i in range(shift, shift+person): ## caricate le observation matrix delta e le starting matrix delta relative agli speaker in esame\n",
    "            matrix_delta.append(np.array(pd.read_csv(BASE_PATH + \"k\" + str(n_cluster)+ \"\\\\delta\\\\matrix\\\\quan_mat_\"+str(i) + \".csv\", header=None), dtype=float))\n",
    "            starting_matrix_delta.append(np.array(pd.read_csv(BASE_PATH + \"k\" + str(n_cluster)+ \"\\\\delta\\\\matrix\\\\start_mat_\"+str(+i)+\".csv\", header=None), dtype=float))\n",
    "\n",
    "        test_set_delta = np.array(pd.read_csv(BASE_PATH +\"k\" + str(n_cluster)+ \"\\\\delta\\\\test_set_hmm_delta.csv\"), dtype=int) ## caricato il test-set delta\n",
    "        test_set_delta = np.delete(test_set_delta, 0, axis=1)\n",
    "\n",
    "\n",
    "        corrected = 0\n",
    "        test_set_frames_mfcc = test_set_mfcc[shift*5: (shift*5)+5*person]\n",
    "        test_set_frames_delta = test_set_delta[shift*5: (shift*5)+5*person]\n",
    "\n",
    "        for audio in range(5*person): ## testato tutti gli audio del testset, relativo al numero di skeaper in esame\n",
    "            sum_prediction = []\n",
    "            current = 0\n",
    "            for frame in range(1,130): ## divisione in finestre\n",
    "                scores_mfcc = []\n",
    "                scores_delta = []\n",
    "\n",
    "                for n_model in range(person):\n",
    "                    scores_mfcc.append(find_probability(matrix[n_model], starting_matrix[n_model],test_set_frames_mfcc[audio][current:frame*20], current)) ## calcolo log probability sulla finestra n-esima della sequenza mfcc\n",
    "\n",
    "                for n_model in range(person):\n",
    "                    scores_delta.append(find_probability(matrix_delta[n_model], starting_matrix_delta[n_model], test_set_frames_delta[audio][current:frame*20], current)) ## calcolo log probability sulla finestra n-esima della sequenza delta\n",
    "\n",
    "                predict_mfcc = max(scores_mfcc) ## scelta la max probability tra tutte le prediction\n",
    "                predict_delta = max(scores_delta)\n",
    "\n",
    "                index_best_mfcc = scores_mfcc.index(predict_mfcc) ## calcolata la prediction relativa alla probabilitá\n",
    "                index_best_delta = scores_delta.index(predict_delta)\n",
    "\n",
    "                sum_prediction.append(index_best_mfcc)\n",
    "                sum_prediction.append(index_best_delta)\n",
    "\n",
    "                current+=20\n",
    "\n",
    "            occurrence = Counter(sum_prediction) ## scelta la moda delle prediction\n",
    "            if correct_label[audio] == occurrence.most_common(1)[0][0]: corrected+=1 \n",
    "        mean.append(corrected/(5*person))\n",
    "        #print(corrected/(5*person))\n",
    "\n",
    "\n",
    "    print(\"n_person = \", person,\" sequence = \", mean,\" max = \", max(mean),\" mean = \", np.mean(mean))\n",
    "    value_nspeaker.append(np.mean(mean))\n",
    "\n",
    "print(value_nspeaker)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}